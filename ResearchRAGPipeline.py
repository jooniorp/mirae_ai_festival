import re
import fitz
import time
import uuid
import os
import http.client
import json
import chromadb
from typing import List
from dotenv import load_dotenv
from pathlib import Path
from tqdm import tqdm
from langchain_chroma import Chroma
from langchain_community.embeddings import ClovaXEmbeddings
from langchain_community.chat_models import ChatClovaX
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain.schema.runnable import RunnableParallel

class CLOVAStudioExecutor:
    def __init__(self, host, api_key):
        self._host = host
        self._api_key = api_key

    def _send_request(self, request, endpoint):
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self._api_key}',
            'X-NCP-CLOVASTUDIO-REQUEST-ID': str(uuid.uuid4())
        }
        conn = http.client.HTTPSConnection(self._host)
        conn.request('POST', endpoint, json.dumps(request), headers)
        response = conn.getresponse()
        result = json.loads(response.read().decode('utf-8'))
        conn.close()
        return result
    
class ResearchRAGPipeline:
    def __init__(self, db_path, collection_name):
        load_dotenv(override=True)
        self.embedding_model = ClovaXEmbeddings(model="bge-m3")
        self.client = chromadb.PersistentClient(path=db_path)
        self.collection_name = collection_name
        self.vectorstore = Chroma(
            client=self.client,
            collection_name=self.collection_name,
            embedding_function=self.embedding_model
        )
        self.segmenter = CLOVAStudioExecutor(
            host="clovastudio.stream.ntruss.com",
            api_key=os.getenv("NCP_CLOVASTUDIO_API_KEY")
        )
        self.documents = []

    def _extract_metadata_from_text(self, text: str):
        metadata = {
            "company": "",
            "date": "",
            "opinion": "",
            "analyst": "",
            "price_target": ""
        }
        company_match = re.search(r"([ê°€-í£]+(?:ì „ì|í™”í•™|ë°”ì´ì˜¤|ì—ë„ˆì§€|ì†”ë£¨ì…˜|ê±´ì„¤|ì¦ê¶Œ|ì€í–‰|ì œì•½))", text)
        if company_match:
            metadata["company"] = company_match.group(1)

        date_match = re.search(r"(\d{4}[.\-ë…„ ]\d{1,2}[.\-ì›” ]\d{1,2})", text)
        if date_match:
            metadata["date"] = date_match.group(1).replace("ë…„", "-").replace("ì›”", "-").replace(" ", "-").replace(".", "-").strip("-")

        opinion_match = re.search(r"(ë§¤ìˆ˜|ì¤‘ë¦½|ë§¤ë„)", text)
        if opinion_match:
            metadata["opinion"] = opinion_match.group(1)

        analyst_match = re.search(r"(?:ì‘ì„±ì|ì• ë„ë¦¬ìŠ¤íŠ¸)[\s:]*([ê°€-í£]+)", text)
        if analyst_match:
            metadata["analyst"] = analyst_match.group(1)

        price_match = re.search(r"ëª©í‘œì£¼ê°€[^\d]*(\d{2,3}(?:,\d{3})*(?:\.\d{1,2})?)", text)
        if price_match:
            metadata["price_target"] = price_match.group(1).replace(",", "")

        return metadata

    def extract_from_pdf_folder(self, folder="./pdf_downloads"):
        path = Path(folder)
        if not path.exists() or not path.is_dir():
            raise ValueError("PDF í´ë” ê²½ë¡œê°€ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")

        data_json = []
        merged_text = ""

        for file in path.glob("*.pdf"):
            print(f"PDF ì²˜ë¦¬ ì¤‘: {file.name}")
            text = ""
            doc = fitz.open(str(file))
            for page in doc:
                text += re.sub(r'\s+', ' ', page.get_text()).strip() + "\n"
            doc.close()

            metadata = self._extract_metadata_from_text(text)
            document = Document(
                page_content=text.strip(),
                metadata=metadata
            )
            self.documents.append(document)

            # JSONìš© êµ¬ì¡° ì¶”ê°€
            data_json.append({
                "file_name": file.name,
                "content": text.strip(),
                "metadata": metadata
            })

            # í…ìŠ¤íŠ¸ íŒŒì¼ ëˆ„ì 
            merged_text += f"\n\n[íŒŒì¼ëª…: {file.name}]\n{text.strip()}"

        print(f"ì´ {len(self.documents)}ê°œ PDF ë¬¸ì„œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

        # ì €ì¥ ê²½ë¡œ ë³´ì¥
        os.makedirs("./data", exist_ok=True)

        # JSON ì €ì¥
        with open("./data/research.json", "w", encoding="utf-8") as f_json:
            json.dump(data_json, f_json, ensure_ascii=False, indent=2)
            print("research.json ì €ì¥ ì™„ë£Œ")

        # TXT ì €ì¥
        with open("./data/merged_research_text.txt", "w", encoding="utf-8") as f_txt:
            f_txt.write(merged_text.strip())
            print("merged_research_text.txt ì €ì¥ ì™„ë£Œ")

    def segment_documents(self):
        if not self.documents:
            raise ValueError("PDFì—ì„œ ë¬¸ì„œë¥¼ ë¨¼ì € ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤. (self.documentsê°€ ë¹„ì–´ ìˆìŒ)")

        segmented_docs = []
        for doc in tqdm(self.documents, desc="CLOVA ì„¸ê·¸ë©˜í…Œì´ì…˜ ì¤‘"):
            try:
                segments = self.segmenter._send_request(
                    request={
                        "postProcessMaxSize": 5000,
                        "alpha": 0.0,
                        "segCnt": -1,
                        "postProcessMinSize": 2000,
                        "text": doc.page_content,
                        "postProcess": True
                    },
                    endpoint="/testapp/v1/api-tools/segmentation"
                )["result"]["topicSeg"]

                overlap = 1  # 1ê°œì˜ ì„¸ê·¸ë¨¼íŠ¸ì”© ê²¹ì¹˜ê²Œ
                for i in range(len(segments)):
                    seg_group = segments[i : i + 1 + overlap]
                    if not seg_group:
                        continue
                    combined = " ".join(" ".join(seg) if isinstance(seg, list) else seg for seg in seg_group)
                    segmented_docs.append(Document(
                        page_content=combined.strip(),
                        metadata=doc.metadata
                    ))
                    
            except Exception as e:
                print(f"segmentation ì‹¤íŒ¨: {e}")

        self.documents = segmented_docs
        print(f"ì´ {len(self.documents)}ê°œì˜ segment ë¬¸ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def embed_and_store(self):
        if not self.documents:
            raise ValueError("segmentation ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € segment_documents()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

        texts, metadatas = [], []
        for i, doc in enumerate(self.documents):
            text = doc.page_content

            if text is None:
                text = ""
            if not isinstance(text, str):
                text = str(text)
            text = text.replace("\x00", "").strip()

            if not text:
                continue

            meta = doc.metadata
            flattened = {
                "id": str(uuid.uuid4()),
                "company": meta.get("company", ""),
                "opinion": meta.get("opinion", ""),
                "date": meta.get("date", ""),
                "analyst": meta.get("analyst", ""),
                "price_target": meta.get("price_target", "")
            }
            flattened = {k: v for k, v in flattened.items() if v}

            texts.append(text)
            metadatas.append(flattened)

        # ì €ì¥
        success = 0
        for i, (text, meta) in enumerate(tqdm(zip(texts, metadatas), desc="ì„ë² ë”© ë° ì €ì¥", total=len(texts))):
            try:
                embedding = self.embedding_model.embed_documents([text])[0]
                self.vectorstore.add_texts(
                    texts=[text],
                    metadatas=[meta],
                    ids=[f"doc_{i}"]
                )
                success += 1
            except Exception as e:
                if "429" in str(e):
                    print(f"429 Too Many Requests â†’ 10ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„ (doc_{i})")
                    time.sleep(10.0)
                    try:
                        embedding = self.embedding_model.embed_documents([text])[0]
                        self.vectorstore.add_texts(
                            texts=[text],
                            metadatas=[meta],
                            ids=[f"doc_{i}_retry"]
                        )
                        success += 1
                    except Exception as e2:
                        print(f"ì¬ì‹œë„ ì‹¤íŒ¨ (doc_{i}): {e2}")
                else:
                    print(f"ì €ì¥ ì‹¤íŒ¨ (doc_{i}): {e}")
            time.sleep(2.0)
            if i % 3 == 0:
                import gc
                gc.collect()

        print(f"\nì´ {success}ê°œ ë¬¸ì„œê°€ ChromaDBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def query(self, question: str) -> str:
        if self.vectorstore is None:
            raise ValueError("vector storeê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € embed_and_store()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

        retriever = self.vectorstore.as_retriever(search_kwargs={"k": 5})  # ğŸ”§ ë¬¸ì„œ ìˆ˜ ì œí•œ

        prompt = PromptTemplate.from_template(
            """ë‹¹ì‹ ì€ ê¸ˆìœµ ì „ë¬¸ê°€ì´ì íˆ¬ì ì‹¬ë¦¬ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
                ì•„ë˜ì— ì œê³µëœ ë¬¸ë§¥(context)ì€ íŠ¹ì • ì¢…ëª©ì— ëŒ€í•œ ì „ë¬¸ê°€ ë¦¬ì„œì¹˜ ë³´ê³ ì„œì…ë‹ˆë‹¤.
                ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì´ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ìš”ì•½ëœ ë¶„ì„ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

                ë‹µë³€ ì‹œ ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¥´ì„¸ìš”:
                - ì •ë³´ì˜ ì¶œì²˜ê°€ ëª…í™•í•˜ì§€ ì•Šìœ¼ë©´ "ì •í™•í•œ ì •ë³´ê°€ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤"ë¼ê³  ë§í•˜ì„¸ìš”.
                - í•µì‹¬ ìš”ì ì„ ê°„ê²°í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”.
                - ìˆ«ìë‚˜ íšŒì‚¬ëª…ì€ ì •í™•íˆ ë°˜ë³µí•˜ê³ , ê·¼ê±°ê°€ ë¶€ì¡±í•œ ì˜ˆì¸¡ì€ í”¼í•˜ì„¸ìš”.

                # Question:
                {question}

                # Context:
                {context}

                # Answer:"""
                )

        def format_docs(docs: List[Document]) -> str:
            return "\n\n".join(doc.page_content[:1500] for doc in docs)  # ë¬¸ì„œ ê¸¸ì´ ì œí•œ

        rag_chain = (
            RunnablePassthrough.assign(context=(lambda x: format_docs(x["context"])))
            | prompt
            | ChatClovaX(model="HCX-003", max_tokens=2048)
            | StrOutputParser()
        )

        rag_pipeline = RunnableParallel(
            {"context": retriever, "question": RunnablePassthrough()}
        ).assign(answer=rag_chain)

        result = rag_pipeline.invoke(question)
        return result["answer"]


def main():
    rag = ResearchRAGPipeline(
        db_path="./chroma_langchain_db",
        collection_name="clovastudiodatas_research_docs"
    )
    rag.extract_from_pdf_folder("./pdf_downloads")
    rag.segment_documents()
    rag.embed_and_store()

    question = "ì‚¼ì„±ì „ìì˜ ëª©í‘œì£¼ê°€ëŠ” ì–¼ë§ˆì¸ê°€ìš”?"
    answer = rag.query(question)
    print(f"\nì§ˆë¬¸: {question}")
    print("ì‘ë‹µ:", answer)


if __name__ == "__main__":
    main()
